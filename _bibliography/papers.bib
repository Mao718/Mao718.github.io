@misc{poyuan2023syntheticshiftsinitialseed,
      title={Synthetic Shifts to Initial Seed Vector Exposes the Brittle Nature of Latent-Based Diffusion Models}, 
      author={Mao Po-Yuan and Shashank Kotyan and Tham Yik Foong and Danilo Vasconcellos Vargas},
      year={2023},
      eprint={2312.11473},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.11473}, 
}
@misc{foong2023challengesimagegenerationmodels,
      title={The Challenges of Image Generation Models in Generating Multi-Component Images}, 
      author={Tham Yik Foong and Shashank Kotyan and Po Yuan Mao and Danilo Vasconcellos Vargas},
      year={2023},
      eprint={2311.13620},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.13620}, 
}
@misc{foong2024adaptingcovariateshiftrealtime,
      title={Adapting to Covariate Shift in Real-time by Encoding Trees with Motion Equations}, 
      author={Tham Yik Foong and Heng Zhang and Mao Po Yuan and Danilo Vasconcellos Vargas},
      year={2024},
      eprint={2404.05168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.05168}, 
}

@misc{kotyan2024breakingfreehacksafety,
      title={Breaking Free: How to Hack Safety Guardrails in Black-Box Diffusion Models!}, 
      author={Shashank Kotyan and Po-Yuan Mao and Pin-Yu Chen and Danilo Vasconcellos Vargas},
      year={2024},
      eprint={2402.04699},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.04699}, 
      selected  = {true},
      html  = {https://shashankkotyan.github.io/EvoSeed/},
      bibtex_show={true}
}
@InProceedings{Dat_2025_WACV,
    abbr={Oral},
    author    = {Dat, Do Huu and Mao, Po-Yuan and Nguyen, Tien Hoang and Buntine, Wray and Bennamoun, Mohammed},
    title     = {HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts},
    booktitle = {Proceedings of the Winter Conference on Applications of Computer Vision (WACV)},
    month     = {February},
    year      = {2025},
    pages     = {1101-1110},
    selected  = {true},
    bibtex_show={true}
}

@article{MAO2023126508,
  title = {Magnum: Tackling high-dimensional structures with self-organization},
  journal = {Neurocomputing},
  volume = {550},
  pages = {126508},
  year = {2023},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2023.126508},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231223006318},
  author = {Po-Yuan Mao and Yikfoong Tham and Heng Zhang and Danilo Vasconcellos Vargas},
  keywords = {Dynamical system based machine learning, Optimization-free, Unsupervised learning, High-dimensional problems, New AI paradigm},
  abstract = {A big challenge in dealing with real-world problems is scalability. In fact, this is partially the reason behind the success of deep learning over other learning paradigms. Here, we tackle the scalability of a novel learning paradigm proposed in 2021 based solely on self-organizing principles. This paradigm consists of only dynamical equations which self-organize with the input to create attractor-repeller points that are related to the patterns found in data. To achieve scalability for such a system, we propose the Magnum algorithm, which utilizes many self-organizing subsystems (Inertia-SyncMap) each with subsets of the problemâ€™s variables. The main idea is that by merging Inertia-SyncMaps, Magnum builds over time a variable correlation by consensus, capable of accurately predicting the structure of large groups of variables. Experiments show that Magnum surpasses or ties with other unsupervised algorithms in all of the high-dimensional chunking problems, each with distinct types of shapes and structural features. Moreover, Inertia-SyncMap alone outperforms or ties with other unsupervised algorithms in six out of seven basic chunking problems. Thus, this work sheds light on how self-organization learning paradigms can be scaled up to deal with high-dimensional structures and compete with current learning paradigms.},
  selected={true},
  bibtex_show={true}
   }